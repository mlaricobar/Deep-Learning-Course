{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NOT01_Introducción_a_Pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlaricobar/Deep-Learning-Course/blob/master/NOT01_Introducci%C3%B3n_a_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq9EAlFMBysC",
        "colab_type": "text"
      },
      "source": [
        "# Introducción a Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riVZ5v1iB21A",
        "colab_type": "text"
      },
      "source": [
        "## 1. Acerca de Pytorch\n",
        "\n",
        "Bienvenidos! En este módulo aprenderás a cómo usar Pytorch para poder construir modelos de Deep Learning. Pytorch fue lanzado a inicios del 2017 y ha tenido un gran impacto en la comunidad del Deep Learning. Fue desarrollado como un proyecto **open source** por el equipo de investigación de [Facebook AI](https://research.fb.com/category/facebook-ai-research/), pero que está siendo adoptada  por equipos de todo el mundo en la industria y en lo académico. \n",
        "\n",
        "Pytorch es considerado el mejor framework para aprender Deep Learning y es muy cómodo trabajar en general. Al final de este módulo, crearemos nuestro propio modelo de Deep Learning para clasificar imágenes de perros y gatos.\n",
        "\n",
        "**Pytorch** se comporta de muchas maneras como los arreglos que has visto en Numpy. Estos arreglos de Numpy, después de todo, son sólo **tensors**. Pytorch toma estos tensores y hace que sea sencillo moverlos a las GPUs para el procesamiento más rápido que se necesita al entrenar redes neuronales. También proporciona un módulo que calcula automáticamente los gradientes (para el backpropagation) y otro módulo específicamente para la construcción de redes neuronales. En general, Pytorch termina siendo más coherente con Python y el stack Numpy/Scipy en comparación con Tensorflow y otros marcos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6587EwefKjPW",
        "colab_type": "text"
      },
      "source": [
        "### Redes Neuronales\n",
        "\n",
        "Deep Learning se basa en las redes neuronales artificiales que han existido de alguna forma desde finales de 1950. Las redes se construyen a partir de partes individuales que se aproximan a las neuronas, típicamente llamadas **unidades** o simplemente **neuronas**. Cada unidad tiene algún número de entradas ponderadas. Estas entradas ponderadas se suman (una combinación lineal) y luego se pasan a través de una función de activación para obtener la salida de la unidad.\n",
        "\n",
        "![texto alternativo](https://github.com/mikolarico/course-images/blob/master/simple_neuron.png?raw=true)\n",
        "\n",
        "Matemáticamente se ve de esta forma:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
        "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "A continuación se muestra el producto escalar de los vectores: \n",
        "\n",
        "$$\n",
        "h = \\begin{bmatrix}\n",
        "x_1 \\, x_2 \\cdots  x_n\n",
        "\\end{bmatrix}\n",
        "\\cdot \n",
        "\\begin{bmatrix}\n",
        "           w_1 \\\\\n",
        "           w_2 \\\\\n",
        "           \\vdots \\\\\n",
        "           w_n\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzsjrSs6DrXb",
        "colab_type": "text"
      },
      "source": [
        "## 2. Tensors\n",
        "\n",
        "Resulta que los cálculos de las redes neuronales son sólo un grupo de operaciones de algebra lineal sobre tensores, una generalización de las matrices. Por ejemplo, un vector es un tensor unidimensional, una matriz es un tensor bidimensional, una matriz con tres índices es un tensor tridimensional (por ejemplo las imágenes en color RGB). La estructura de datos fundamental para las redes neuronales son los tensores y Pytorch (así como casi todos los demás frameworks de Deep Learning) se basa en ellos.\n",
        "\n",
        "![texto alternativo](https://github.com/mikolarico/course-images/blob/master/tensor_examples.png?raw=true)\n",
        "\n",
        "Ya con lo básico cubierto, es hora de explorar cómo podemos usar Pytorch para construir una simple red neuronal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke35UgH3Bl2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Primero importamos la librería de Pytorch\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQjqlgSvRkjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def activation(x):\n",
        "  \"\"\" Función de activación Sigmoide\n",
        "  \n",
        "      Argumentos\n",
        "      ------------\n",
        "      x: torch.Tensor\n",
        "  \"\"\"\n",
        "  return 1 / (1 + torch.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoIg0uCIR144",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Generaremos algunos datos\n",
        "torch.manual_seed(7) # Establecemos el valor de la semilla para replicar los resultados\n",
        "\n",
        "# Features son 3 variables aleatorias con distribución normal\n",
        "features = torch.randn((1, 5))\n",
        "\n",
        "# Pesos para nuestros datos, variables normales aleatorias nuevamente\n",
        "weights = torch.randn_like(features)\n",
        "\n",
        "# y el sesgo\n",
        "bias = torch.randn((1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTc4yEXKgQxP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "70174377-764e-4e5b-c043-d64a69baad52"
      },
      "source": [
        "features"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eapqeB2lgSYI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5e4332e5-6e77-48e9-c253-af06e910934f"
      },
      "source": [
        "weights"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmwRl-ZQgUEa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c665de28-7f78-4b10-f79c-12b4c6b15b92"
      },
      "source": [
        "bias"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3177]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbLqZM-qkuZM",
        "colab_type": "text"
      },
      "source": [
        "En las líneas anteriores, se generaron datos que podemos usar para obtener la salida de nuestra simple red. Todo es aleatorio por ahora, en el futuro comenzaremos a usar datos normales. Iremos paso a paso en cada línea:\n",
        "\n",
        "`features = torch.randn((1, 5))` crea un tensor con dimensiones `(1, 5)`, una fila y 5 columnas, que contiene valores distribuidos aleatoriamente según la distribución normal con una media de cero y una desviación estándar de uno.\n",
        "\n",
        "`weights = torch.randn_like(features)` crea otro tensor con las mismas dimensiones que `features`, nuevamente contiene los valores de una distribución normal.\n",
        "\n",
        "Finalmente `bias = torch.randn((1, 1))` crea un único valor a partir de una distribución normal.\n",
        "\n",
        "Los tensores de Pytorch se pueden sumar, multiplicar, restar, etc, al igual que los arreglos de Numpy. En general, utilizarás Pytorch de la misma forma que con las matrices de Numpy. Vienen con algunos buenos beneficios, como la aceleración de GPY, que veremos más adelante. Por ahora, use los datos generados para calcular la salida de esta simple red de una sola capa.\n",
        "\n",
        "> **Ejercicio**: Calcule la salida de la red con las variables de entrada `features`, pesos `weights` y sesgo `bias`. Similar a Numpy, Pytorch tiene una función [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum) así como el método `.sum()` en los tensores, para sumarlos. Use la función `activation` definida anteriormente como la función de activación.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVI-n4JgqYzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3614ca9e-1554-4086-d075-8c241a58fe1c"
      },
      "source": [
        "## Calcular la salida de la red usando los tensores de pesos y sesgo\n",
        "y = activation(torch.sum(features * weights) +  bias)\n",
        "y"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1595]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjimpvQS03XX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "b22e4119-1df3-4f25-d5fb-fce51db1d0e9"
      },
      "source": [
        "y = activation((features * weights).sum() +  bias)\n",
        "y"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1595]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcNLEAEpq4Ae",
        "colab_type": "text"
      },
      "source": [
        "Puedes hacer la multiplicación y la suma en la misma operación usando una multiplicación de matrices. En general, queremos usar las multiplicaciones de matrices, ya que son más eficientes y aceleradas al usar librerías modernas y cálculo de alto rendimiento en las GPUs.\n",
        "\n",
        "Aquí, queremos hacer una multiplicación matricial de las características y los pesos. Para esto podemos usar [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) o [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul) que es algo más complicado y soporta el broadcasting. Si intentamos hacerlo con `features` y `weight` como están, obtendremos un error\n",
        "\n",
        "```python\n",
        ">> torch.mm(features, weights)\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "RuntimeError                              Traceback (most recent call last)\n",
        "<ipython-input-13-15d592eb5279> in <module>()\n",
        "----> 1 torch.mm(features, weights)\n",
        "\n",
        "RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033\n",
        "```\n",
        "\n",
        "Cuando construyas redes neuronales en cualquier framework, verás esto a menudo. Muy a menudo. Lo que sucede aquí es que nuestros tensores no tienen las dimensiones correctas para realizar una multiplicación de matrices. Recuerde que para las multiplicaciones matriciales, el número de columnas en el primer tensor debe ser igual al número de filas en el segundo tensor. Tanto `features` como `weights` tienen la misma dimensión, `(1, 5)`. Esto significa que necesitamos cambiar la forma de `weights` para que la multiplicación de matrices funcione.\n",
        "\n",
        "**Nota:** Para ver la forma de un tensor llamado `tensor`, use `tensor.shape`. Si te encuentras construyendo redes neuronales, a menudo utilizarás este método.\n",
        "\n",
        "Hay algunas opciones aquí: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), y [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
        "\n",
        "* `weight.reshape(a, b)` devolverá un nuevo tensor con los mismos datos que `weights` pero con tamaño `(a, b)` a veces, y algunas veces un clon, ya que copia los datos a otra parte de memoria.\n",
        "* `weights.resize_(a, b)` devuelve el mismo tensor con una forma diferente. Sin embargo, si la nueva forma produce menos elementos que el tensor original, algunos elementos se eliminarán del tensor (pero no de la memoria). Si la nueva forma produce más elementos que el tensor original, los nuevos elementos se quedarán sin inicializar en la memoria. Aquí debo señalar que el subrayado al final del método denota que este método se realizar **en el lugar**. Aquí hay un hilo de foro para [leer más sobre las operaciones en el lugar](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) en Pytorch.\n",
        "* `weight.view(a, b)` devolverá un nuevo tensor con los mismos datos que `weights` pero con tamaño `(a, b)`.\n",
        "\n",
        "Usualmente uso `.view()`, pero cualquiera de los tres métodos funcionará para esto. Entonces, ahora podemos cambiar la forma de `weights`  para tener cinco filas y una columna usando `weights.view(5, 1)`.\n",
        "\n",
        "> **Ejercicio**: Calcule la salida de nuestra pequeña red usando la multiplicación de matrices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeR9twCk0ZUN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5ac69bd4-de61-456a-fbec-12202cc20c6e"
      },
      "source": [
        "features"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM1NOE7i2X0R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "10dbafa4-9007-438f-9b05-e6e1c9778963"
      },
      "source": [
        "weights.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tYBmA1j2Xxt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "bbfeff91-77b4-4e8d-de79-0addabbb99e4"
      },
      "source": [
        "weights.view(5, 1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8948],\n",
              "        [-0.3556],\n",
              "        [ 1.2324],\n",
              "        [ 0.1382],\n",
              "        [-1.6822]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2sn-1V92Xu-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "4dbdfef8-99e5-44dc-ef4e-662de0d72a15"
      },
      "source": [
        "## Calcula la salida de esta red usando la multiplicación de matrices\n",
        "y = activation(torch.mm(features, weights.view(5, 1)) + bias)\n",
        "y"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1595]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7WMdgcI25XU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "6ecb18aa-a316-4ab1-a580-87392f192dbc"
      },
      "source": [
        "## Calcula la salida de esta red usando la multiplicación de matrices\n",
        "y = activation(torch.matmul(features, weights.view(5, 1)) + bias)\n",
        "y"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1595]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6GcXQS24guU",
        "colab_type": "text"
      },
      "source": [
        "### Stack them up!\n",
        "\n",
        "Así es como se puede calcular la salida para una sola neurona. El poder real de este algoritmo sucede cuando comienza a apilar estas unidades individuales en capas y estas  capas en una red de neuronas. La salida de una capa de neuronas se convierte en la entrada para la siguiente capa. Con múltiples unidades de entrada y unidades de salida, ahora necesitamos expresar los pesos como una matriz.\n",
        "\n",
        "![texto alternativo](https://github.com/mikolarico/course-images/blob/master/multilayer_diagram_weights.png?raw=true)\n",
        "\n",
        "La primera capa que se muestra en la parte inferior son las entradas, comprensiblemente llamadas **capa de entrada**. La capa intermedia se llama **capa oculta**, y la capa final es la **capa de salida**. Podemos expresar esta red matemáticamente de nuevo con matrices y usar la multiplicación de matrices para obtener combinaciones lineales para cada unidad en una sola operación. Por ejemplo, la capa oculta ($h_1$ y $h_2$) se puede calcular:\n",
        "\n",
        "\n",
        "$$\n",
        "\\vec{h} = [h_1 \\, h_2] = \n",
        "\\begin{bmatrix}\n",
        "x_1 \\, x_2 \\cdots \\, x_n\n",
        "\\end{bmatrix}\n",
        "\\cdot \n",
        "\\begin{bmatrix}\n",
        "           w_{11} & w_{12} \\\\\n",
        "           w_{21} &w_{22} \\\\\n",
        "           \\vdots &\\vdots \\\\\n",
        "           w_{n1} &w_{n2}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "La salida para esta pequeña red se encuentra al tratar la capa oculta como entradas para la unidad de salida. La salida de la red se expresa simplemente como\n",
        "\n",
        "$$\n",
        "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQvo4uhM4f64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Generaremos algunos datos\n",
        "torch.manual_seed(7) # Establecemos el valor de la semilla para replicar los resultados\n",
        "\n",
        "# Features son 3 variables aleatorias con distribución normal\n",
        "features = torch.randn((1, 3))\n",
        "\n",
        "# Definimos el tamaño de cada capa en nuestra red\n",
        "n_input = features.shape[1]     # Número de unidades de entrada, debe coincidir con el número de variables de entrada\n",
        "n_hidden = 2                    # Número de unidades ocultas\n",
        "n_output = 1                    # Número de unidades de salida\n",
        "\n",
        "# Pesos para las entradas hacia la capa oculta\n",
        "W1 = torch.randn(n_input, n_hidden)\n",
        "\n",
        "# Pesos para la capa oculta hacia la capa de salida\n",
        "W2 = torch.randn(n_hidden, n_output)\n",
        "\n",
        "# y el sesgo para las capas oculta y la de salida\n",
        "B1 = torch.randn((1, n_hidden))\n",
        "B2 = torch.randn((1, n_output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiBgtoYkK2EW",
        "colab_type": "text"
      },
      "source": [
        "> **Ejercicio:** Calcule la salida para esta red de múltiples capas utilizando los pesos `W1` y `W2`, y los sesgos `B1` y `B2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnBZPsZWLQN6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "bb0dcdfe-ff55-4d4d-b072-eef329f710be"
      },
      "source": [
        "features"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1468,  0.7861,  0.9468]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-KuwtcRLQHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "329dac5b-3947-41e2-d227-775f56a2897c"
      },
      "source": [
        "W1"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.1143,  1.6908],\n",
              "        [-0.8948, -0.3556],\n",
              "        [ 1.2324,  0.1382]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0TRLZmkFGyc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "db01c751-a62e-488c-8b2f-05c32e6b3225"
      },
      "source": [
        "# Escribe aquí tu solución\n",
        "activation(torch.matmul(activation(torch.matmul(features, W1) + B1), W2) + B2)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3171]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kICiaImTNaGT",
        "colab_type": "text"
      },
      "source": [
        "El número de unidades ocultas es un parámetro de la red, a menudo llamado un **hiperparámetro** para diferenciarlo de los parámetros `weights` y `biases`. Como veremos más adelante, cuando hablemos sobre la formación de una red neuronal, cuantas más unidades ocultas tenga una red y cuantas más capas, mejor podrá aprender de los datos y hacer predicciones precisas. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpvcAo6EDuGT",
        "colab_type": "text"
      },
      "source": [
        "## 3. Autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClvSj2zaDKYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkTZlrCWDwrk",
        "colab_type": "text"
      },
      "source": [
        "## 4. Construcción de una Red Neuronal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZDnrqu2D5XC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waJckXDvEEsv",
        "colab_type": "text"
      },
      "source": [
        "## 5. Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55UPE5JUEGfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbTf5YZAFh7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}